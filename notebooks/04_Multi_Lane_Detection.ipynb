{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "\n",
    "- In out problem, need to detect the nearest pedestrian road to see if the car is violating or not \n",
    "- need to know which part is the pedestrian road and which part is the car road  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = Path(\".\").absolute().parent\n",
    "# data_path = Path(\".\") / \"videos\"\n",
    "data_path = Path(\"D:\\Datas\\parking_violation\")\n",
    "pkg_path = repo_path.parent / \"LaneATT\"\n",
    "cfg_path = pkg_path / \"cfgs\" / \"test.yml\"\n",
    "\n",
    "sys.path.append(str(repo_path))\n",
    "sys.path.append(str(pkg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lanedetection import imreader, imsaver\n",
    "from lib.config import Config\n",
    "from lib.models import LaneATT\n",
    "from lib.runner import Runner\n",
    "from lib.datasets.lane_dataset import LaneDataset\n",
    "from lanedetection import check_rotation, correct_rotation, correct_rgb, resize_image\n",
    "\n",
    "height, width = 360, 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(cfg_path)\n",
    "device = torch.device('cpu') # if not torch.cuda.is_available() or args.cpu else torch.device('cuda')\n",
    "runner = Runner(cfg, \"test\", device, view=\"all\", resume=False, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = str(data_path / \"sample1.mp4\")\n",
    "sv_path = repo_path / \"notebooks\" / \"imgs\" / \"sample1_fps30\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "rotate_code = check_rotation(video_path)\n",
    "\n",
    "frame_rate = 30\n",
    "prev = 0\n",
    "count = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    time_elapsed = time.time() - prev\n",
    "    if ret & (time_elapsed > 1./frame_rate):\n",
    "        prev = time.time()\n",
    "        # process image\n",
    "        if rotate_code:\n",
    "            frame = correct_rotation(frame, rotate_code)\n",
    "        frame = resize_image(frame, width, height)\n",
    "        # frame = correct_rgb(frame)\n",
    "        # pipeline\n",
    "        images = frame\n",
    "        \n",
    "        cv2.imwrite( str(sv_path / f\"{count:05d}.jpg\"), images)\n",
    "        count += 1\n",
    "        \n",
    "        cv2.imshow(\"video\", images)\n",
    "        key = cv2.waitKey(5)\n",
    "        if key == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    if not ret:\n",
    "        break\n",
    "            \n",
    "if not ret:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./imgs/test2.jpg\"\n",
    "img = imreader(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "S = cfg.config[\"datasets\"][\"test\"][\"parameters\"][\"S\"]\n",
    "normalize = False\n",
    "img_size = cfg.config[\"datasets\"][\"test\"][\"parameters\"][\"img_size\"]\n",
    "n_strips = S - 1\n",
    "n_offsets = S\n",
    "normalize = normalize\n",
    "img_h, img_w = img_size\n",
    "strip_size = img_h / n_strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backbone=\"resnet34\"\n",
    "# pretrained_backbone=True\n",
    "# S=72\n",
    "# img_w=width\n",
    "# img_h=height\n",
    "# anchors_freq_path= str(pkg_path / \"data\" / \"culane_anchors_freq.pt\")\n",
    "# topk_anchors=1000\n",
    "# anchor_feat_channels=64\n",
    "model_path = pkg_path / \"experiments\" / \"laneatt_r34_culane\" / \"models\" / \"model_0015.pt\"\n",
    "model = cfg.get_model()\n",
    "model.load_state_dict(torch.load(model_path)[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.FloatTensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "test_parameters = cfg.get_test_parameters()\n",
    "conf_threshold, nms_thres, nms_topk = test_parameters.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\miniconda3\\envs\\venv\\lib\\site-packages\\nms\\malisiewicz.py:66: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (w * h)/area\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-1e05a8b718fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtest_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_lanes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Codes\\LaneATT\\lib\\models\\laneatt.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, conf_threshold, nms_thres, nms_topk)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Apply nms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mproposals_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg_proposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_thres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_topk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mproposals_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Codes\\LaneATT\\lib\\models\\laneatt.py\u001b[0m in \u001b[0;36mnms\u001b[1;34m(self, batch_proposals, batch_attention_matrix, nms_thres, nms_topk, conf_threshold)\u001b[0m\n\u001b[0;32m    127\u001b[0m                     \u001b[0mproposals_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mkeep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_to_keep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnms_thres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnms_topk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                 \u001b[0mkeep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mproposals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs, **test_parameters)\n",
    "prediction = model.decode(outputs, as_lanes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nms import nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nms.nms import boxes, rboxes, polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nms.nms.boxes(rects, scores, nms_algorithm=<function nms at 0x0000022B1FE8E158>, **kwargs)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-1251ecf474ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mproposals_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg_proposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_thres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_topk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Codes\\LaneATT\\lib\\models\\laneatt.py\u001b[0m in \u001b[0;36mnms\u001b[1;34m(self, batch_proposals, batch_attention_matrix, nms_thres, nms_topk, conf_threshold)\u001b[0m\n\u001b[0;32m    127\u001b[0m                     \u001b[0mproposals_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mkeep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_to_keep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnms_thres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnms_topk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                 \u001b[0mkeep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mproposals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\nms\\fast.py\u001b[0m in \u001b[0;36mnms\u001b[1;34m(boxes, scores, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mcompare_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcompare_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "proposals_list = model.nms(reg_proposals, attention_matrix, nms_thres, nms_topk, conf_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "batch_features = model.feature_extractor(x)\n",
    "batch_features = model.conv1(batch_features)\n",
    "batch_anchor_features = model.cut_anchor_features(batch_features)\n",
    "\n",
    "# Join proposals from all images into a single proposals features batch\n",
    "batch_anchor_features = batch_anchor_features.view(-1, model.anchor_feat_channels * model.fmap_h)\n",
    "\n",
    "# Add attention features\n",
    "softmax = nn.Softmax(dim=1)\n",
    "scores = model.attention_layer(batch_anchor_features)\n",
    "attention = softmax(scores).reshape(x.shape[0], len(model.anchors), -1)\n",
    "attention_matrix = torch.eye(attention.shape[1], device=x.device).repeat(x.shape[0], 1, 1)\n",
    "non_diag_inds = torch.nonzero(attention_matrix == 0., as_tuple=False)\n",
    "attention_matrix[:] = 0\n",
    "attention_matrix[non_diag_inds[:, 0], non_diag_inds[:, 1], non_diag_inds[:, 2]] = attention.flatten()\n",
    "batch_anchor_features = batch_anchor_features.reshape(x.shape[0], len(model.anchors), -1)\n",
    "attention_features = torch.bmm(torch.transpose(batch_anchor_features, 1, 2),\n",
    "                               torch.transpose(attention_matrix, 1, 2)).transpose(1, 2)\n",
    "attention_features = attention_features.reshape(-1, model.anchor_feat_channels * model.fmap_h)\n",
    "batch_anchor_features = batch_anchor_features.reshape(-1, model.anchor_feat_channels * model.fmap_h)\n",
    "batch_anchor_features = torch.cat((attention_features, batch_anchor_features), dim=1)\n",
    "\n",
    "# Predict\n",
    "cls_logits = model.cls_layer(batch_anchor_features)\n",
    "reg = model.reg_layer(batch_anchor_features)\n",
    "\n",
    "# Undo joining\n",
    "cls_logits = cls_logits.reshape(x.shape[0], -1, cls_logits.shape[1])\n",
    "reg = reg.reshape(x.shape[0], -1, reg.shape[1])\n",
    "\n",
    "# Add offsets to anchors\n",
    "reg_proposals = torch.zeros((*cls_logits.shape[:2], 5 + model.n_offsets), device=x.device)\n",
    "reg_proposals += model.anchors\n",
    "reg_proposals[:, :, :2] = cls_logits\n",
    "reg_proposals[:, :, 4:] += reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1000, 77]), torch.Size([1, 1000, 1000]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_proposals.shape, attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_proposals = reg_proposals\n",
    "batch_attention_matrix = attention_matrix\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "proposals_list = []\n",
    "for proposals, attention_matrix in zip(batch_proposals, batch_attention_matrix):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 77]), torch.Size([1000, 1000]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals.shape, attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_inds = torch.arange(batch_proposals.shape[1], device=proposals.device)\n",
    "scores = softmax(proposals[:, :2])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_threshold = scores > conf_threshold\n",
    "proposals = proposals[above_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores[above_threshold]\n",
    "anchor_inds = anchor_inds[above_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes(proposals.detach(), scores, overlap=nms_thres, top_k=nms_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 77])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5288, 0.6722, 0.6807, 0.6726, 0.6786, 0.6786, 0.5061, 0.5362, 0.5180],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-1be238432ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mproposals_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mproposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_proposals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_attention_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0manchor_inds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_proposals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproposals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# The gradients do not have to (and can't) be calculated for the NMS procedure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "batch_proposals = reg_proposals\n",
    "batch_attention_matrix = attention_matrix\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "proposals_list = []\n",
    "for proposals, attention_matrix in zip(batch_proposals, batch_attention_matrix):\n",
    "    anchor_inds = torch.arange(batch_proposals.shape[1], device=proposals.device)\n",
    "    # The gradients do not have to (and can't) be calculated for the NMS procedure\n",
    "    with torch.no_grad():\n",
    "        scores = softmax(proposals[:, :2])[:, 1]\n",
    "        if conf_threshold is not None:\n",
    "            # apply confidence threshold\n",
    "            above_threshold = scores > conf_threshold\n",
    "            proposals = proposals[above_threshold]\n",
    "            scores = scores[above_threshold]\n",
    "            anchor_inds = anchor_inds[above_threshold]\n",
    "        if proposals.shape[0] == 0:\n",
    "            proposals_list.append((proposals[[]], model.anchors[[]], attention_matrix[[]], None))\n",
    "            continue\n",
    "        keep, num_to_keep, _ = boxes(proposals, scores, overlap=nms_thres, top_k=nms_topk)\n",
    "        keep = keep[:num_to_keep]\n",
    "    proposals = proposals[keep]\n",
    "    anchor_inds = anchor_inds[keep]\n",
    "    attention_matrix = attention_matrix[anchor_inds]\n",
    "    proposals_list.append((proposals, model.anchors[keep], attention_matrix, anchor_inds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 77])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (images[0].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "img, fp, fn = dataloader.dataset.draw_annotation(idx, img=img, pred=prediction[0])\n",
    "if self.view == 'mistakes' and fp == 0 and fn == 0:\n",
    "    continue\n",
    "cv2.imshow('pred', img)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
