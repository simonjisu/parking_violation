{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy import io as spio\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    repo_path = Path(\".\").absolute().parent\n",
    "    data_path = Path(\"D:/Datas/\") / \"cardataset\"\n",
    "else:    \n",
    "    repo_path = Path(\".\").absolute().parent\n",
    "    data_path = repo_path.parent / \"data\" / \"cardataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = spio.loadmat(data_path / \"cars_annos.mat\")\n",
    "annotations = mat_file[\"annotations\"].reshape(-1)\n",
    "\n",
    "*_, cls, _ = list(zip(*annotations))\n",
    "cls_array = np.array(cls).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_cars = len(np.unique(cls_array))\n",
    "\n",
    "plt.hist(cls_array, bins=n_cars)\n",
    "plt.title(f\"Distributions of {n_cars} cars\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724f756f180047bbbc85f9b9a5a90623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16185.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_preprocessing(annotations):     \n",
    "    # when plotting need: x_min, y_min, x_max, y_max -> y_min, x_min, y_max, x_max\n",
    "\n",
    "    df = [[\"img_path\", \"width\", \"height\", \"class\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
    "    for i, line in tqdm_notebook(enumerate(annotations, 1), total=len(annotations)):\n",
    "        img_path, xmin, ymin, xmax, ymax, cls, _ = map(lambda x: list(x)[0], line)\n",
    "    \n",
    "        image = cv2.imread(str(data_path / img_path))\n",
    "        height, width, _ = image.shape\n",
    "        \n",
    "        df.append([img_path, width, height, int(cls), int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "    return df\n",
    "\n",
    "\n",
    "annotations_clean = np.array(\n",
    "    collect_preprocessing(annotations)\n",
    ")\n",
    "\n",
    "with (data_path / \"car_ims_labels.csv\").open(\"w\") as file:\n",
    "    np.savetxt(file, annotations_clean, fmt=\"%s\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split 10% of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "_, counts = np.unique(cls_array, return_counts=True)\n",
    "test_numbers = (counts * ratio).astype(np.int8)\n",
    "\n",
    "test_indices = []\n",
    "count_idx = 0\n",
    "for total, num in zip(counts, test_numbers):\n",
    "    idx = np.random.choice(np.arange(count_idx, count_idx+total), size=(num,), replace=False)\n",
    "    test_indices.extend(idx.tolist())\n",
    "    count_idx += total\n",
    "    \n",
    "test_indices = np.array(test_indices)\n",
    "test_indices.sort()\n",
    "train_indices = np.arange(counts.sum())[~np.isin(np.arange(counts.sum()), test_indices)]\n",
    "\n",
    "train_dataset = annotations_clean[1:][train_indices]\n",
    "test_dataset = annotations_clean[1:][test_indices]\n",
    "\n",
    "with (data_path / \"car_ims_train_labels.csv\").open(\"w\", encoding=\"utf-8\") as file:\n",
    "    np.savetxt(file, train_dataset, fmt=\"%s\",delimiter=\",\")\n",
    "\n",
    "with (data_path / \"car_ims_train_labels.csv\").open(\"w\", encoding=\"utf-8\") as file:\n",
    "    np.savetxt(file, test_dataset, fmt=\"%s\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create category index and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = mat_file[\"class_names\"].reshape(-1)\n",
    "# class_names = list(map(lambda x: list(x)[0], class_names))\n",
    "# class_names.insert(0, \"__background__\")\n",
    "\n",
    "category_index = {i: {\"id\": i, \"name\": list(x)[0]} for i, x in enumerate(class_names, 1)}\n",
    "cate_dicts = {v[\"id\"]: v[\"name\"] for v in category_index.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
    "from google.protobuf import text_format\n",
    "\n",
    "def convert_classes(classes, start=1):\n",
    "    msg = StringIntLabelMap()\n",
    "    for i, name in enumerate(classes, start=start):\n",
    "        msg.item.append(StringIntLabelMapItem(id=i, name=name))\n",
    "\n",
    "    text = str(text_format.MessageToBytes(msg, as_utf8=True), \"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_path = data_path / \"label_map.pbtxt\"\n",
    "labels = list(cate_dicts.values())\n",
    "txt = convert_classes(labels)\n",
    "with label_map_path.open(\"w\") as file:\n",
    "    file.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert it to tfrecord form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(annotation, cate_dicts, data_path):\n",
    "    \"\"\"\n",
    "    only single object in the image so didn't use group\n",
    "    \"\"\"\n",
    "    img_path = annotation[0]\n",
    "    width, height, cls, xmin, ymin, xmax, ymax = map(int, annotation[1:]) \n",
    "    filename = bytes(img_path.split(\"/\")[-1], encoding=\"utf-8\")\n",
    "    encoded_img = tf.io.gfile.GFile(str(data_path / img_path), \"rb\").read()\n",
    "    \n",
    "    image_format = b\"jpg\"\n",
    "    xmins = [xmin/width]\n",
    "    xmaxs = [xmax/width]\n",
    "    ymins = [ymin/height]\n",
    "    ymaxs = [ymax/height]\n",
    "    classes_text = [bytes(cate_dicts[cls], encoding=\"utf-8\")]\n",
    "    classes = [cls]\n",
    "\n",
    "    feature={\n",
    "        \"image/height\": dataset_util.int64_feature(height),\n",
    "        \"image/width\": dataset_util.int64_feature(width),\n",
    "        \"image/filename\": dataset_util.bytes_feature(filename),\n",
    "        \"image/source_id\": dataset_util.bytes_feature(filename),\n",
    "        \"image/encoded\": dataset_util.bytes_feature(encoded_img),\n",
    "        \"image/format\": dataset_util.bytes_feature(image_format),\n",
    "        \"image/object/bbox/xmin\": dataset_util.float_list_feature(xmins),\n",
    "        \"image/object/bbox/xmax\": dataset_util.float_list_feature(xmaxs),\n",
    "        \"image/object/bbox/ymin\": dataset_util.float_list_feature(ymins),\n",
    "        \"image/object/bbox/ymax\": dataset_util.float_list_feature(ymaxs),\n",
    "        \"image/object/class/text\": dataset_util.bytes_list_feature(classes_text),\n",
    "        \"image/object/class/label\": dataset_util.int64_list_feature(classes),\n",
    "    }\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecode(data_path, cate_dicts, dataset_csv, outputfile):\n",
    "    tf_writer = tf.io.TFRecordWriter(str(data_path / outputfile))\n",
    "    for annotation in dataset_csv:\n",
    "        tf_example = create_tf_example(annotation, cate_dicts, data_path)\n",
    "        tf_writer.write(tf_example.SerializeToString())\n",
    "    tf_writer.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "create_tfrecode(data_path, cate_dicts, dataset_csv=train_dataset, outputfile=\"train.record\")\n",
    "create_tfrecode(data_path, cate_dicts, dataset_csv=test_dataset, outputfile=\"test.record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bbox_x1: Min x-value of the bounding box, in pixels\n",
    "* bbox_x2: Max x-value of the bounding box, in pixels\n",
    "* bbox_y1: Min y-value of the bounding box, in pixels\n",
    "* bbox_y2: Max y-value of the bounding box, in pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"ssd_efficientdet_d0_512x512_coco17_tpu-8.config\"\n",
    "model_name = \"efficientdet_d0_coco17_tpu-32\"\n",
    "\n",
    "pipeline_config = str(repo_path.parent / \"pretrained_model\" / \"configs\" / config_name)\n",
    "checkpoint_path = str(repo_path.parent / \"pretrained_model\" / model_name / \"checkpoint\" / \"ckpt-0\")\n",
    "train_tfrecord_path = str(data_path / \"train.record\")\n",
    "test_tfrecord_path = str(data_path / \"test.record\")\n",
    "label_map_path = str(label_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
    "model_name = \"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\"\n",
    "\n",
    "pipeline_config = str(repo_path.parent / \"pretrained_model\" / \"configs\" / config_name)\n",
    "checkpoint_path = str(repo_path.parent / \"pretrained_model\" / model_name / \"checkpoint\" / \"ckpt-0\")\n",
    "train_tfrecord_path = str(data_path / \"train.record\")\n",
    "test_tfrecord_path = str(data_path / \"test.record\")\n",
    "label_map_path = str(label_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "batch_size = 4\n",
    "n_epochs = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pipline(\n",
    "        old_config_path, \n",
    "        new_config_path, \n",
    "        checkpoint_path, \n",
    "        label_map_path, \n",
    "        train_tfrecord_path, \n",
    "        test_tfrecord_path,\n",
    "        num_classes,\n",
    "        batch_size,\n",
    "        n_epochs\n",
    "    ):\n",
    "    pipeline = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
    "    \n",
    "    with tf.io.gfile.GFile(old_config_path, \"r\") as f:                                                                                                                                                                                                                     \n",
    "        proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "        text_format.Merge(proto_str, pipeline)\n",
    "        \n",
    "    pipeline.model.ssd.num_classes = num_classes\n",
    "    pipeline.model.ssd.freeze_batchnorm = True\n",
    "    \n",
    "    pipeline.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "    pipeline.train_config.use_bfloat16 = False\n",
    "    pipeline.train_config.batch_size = batch_size\n",
    "    pipeline.train_config.num_steps = n_epochs\n",
    "    \n",
    "    pipeline.train_config.fine_tune_checkpoint = checkpoint_path\n",
    "    pipeline.train_input_reader.label_map_path = label_map_path\n",
    "    pipeline.train_input_reader.tf_record_input_reader.input_path[:] = [train_tfrecord_path]\n",
    "    \n",
    "    pipeline.eval_input_reader[0].label_map_path = label_map_path\n",
    "    pipeline.eval_input_reader[0].tf_record_input_reader.input_path[:] = [test_tfrecord_path]\n",
    "    \n",
    "    config_text = text_format.MessageToString(pipeline)\n",
    "    with tf.io.gfile.GFile(new_config_path, \"wb\") as f:                                                                                                                                                                                                                       \n",
    "        f.write(config_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config_folder = repo_path.parent / \"pretrained_model\" / \"mymodel\"\n",
    "if not new_config_folder.exists():\n",
    "    new_config_folder.mkdir()\n",
    "\n",
    "new_config_path = str(new_config_folder / \"ssd_efficientdet_d0_512x512_coco17_custom.config\")\n",
    "#new_config_path = str(new_config_folder / \"ssd_resnet50_v1_fpn_640x640_coco17_custom.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Desktop\\Codes\\pretrained_model\\mymodel\n",
      "C:\\Users\\simon\\Desktop\\Codes\\pretrained_model\\mymodel\\ssd_efficientdet_d0_512x512_coco17_custom.config\n"
     ]
    }
   ],
   "source": [
    "print(new_config_folder)\n",
    "print(new_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_pipline(\n",
    "    old_config_path=pipeline_config, \n",
    "    new_config_path=new_config_path, \n",
    "    checkpoint_path=checkpoint_path, \n",
    "    label_map_path=label_map_path, \n",
    "    train_tfrecord_path=train_tfrecord_path, \n",
    "    test_tfrecord_path=test_tfrecord_path,\n",
    "    num_classes=len(category_index),\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python .\\models\\research\\object_detection\\model_main_tf2.py --model_dir=C:\\Users\\simon\\Desktop\\Codes\\pretrained_model\\mymodel --pipeline_config_path=C:\\\\Users\\\\simon\\\\Desktop\\\\Codes\\\\pretrained_model\\\\mymodel\\\\ssd_efficientdet_d0_512x512_coco17_custom.config\n",
    "```\n",
    "```\n",
    "python .\\models\\research\\object_detection\\model_main_tf2.py --model_dir=C:\\Users\\simon\\Desktop\\Codes\\pretrained_model\\mymodel --pipeline_config_path=C:\\\\Users\\\\simon\\\\Desktop\\\\Codes\\\\pretrained_model\\\\mymodel\\\\ssd_resnet50_v1_fpn_640x640_coco17_custom.config\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=C:\\Users\\simon\\Desktop\\Codes\\pretrained_model\\mymodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "        path: a file path.\n",
    "\n",
    "    Returns:\n",
    "        uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    if isinstance(path, Path):\n",
    "        path = str(path)\n",
    "    return cv2.imread(path).astype(np.uint8)\n",
    "#     img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "#     image = Image.open(BytesIO(img_data))\n",
    "#     (im_width, im_height) = image.size\n",
    "#     return np.array(image.getdata()).reshape(\n",
    "#         (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "    \"\"\"Wrapper function to visualize detections.\n",
    "\n",
    "    Args:\n",
    "        image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "        boxes: a numpy array of shape [N, 4]\n",
    "        classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
    "            and match the keys in the label map.\n",
    "        scores: a numpy array of shape [N] or None.  If scores=None, then\n",
    "            this function assumes that the boxes to be plotted are groundtruth\n",
    "            boxes and plot all boxes as black with no classes or scores.\n",
    "        category_index: a dict containing category dictionaries (each holding\n",
    "            category index `id` and category name `name`) keyed by category indices.\n",
    "        figsize: size for the figure.\n",
    "        image_name: a name for the image file.\n",
    "    \"\"\"\n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        min_score_thresh=0.8)\n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "        ax.imshow(image_np_with_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(len(annotations_dict)))\n",
    "img_path = annotations_dict[idx][\"img_path\"]\n",
    "bbox = annotations_dict[idx][\"bbox\"]\n",
    "cls = annotations_dict[idx][\"class\"]\n",
    "score = np.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_image_into_numpy_array(data_path / img_path)\n",
    "plot_detections(x, boxes=bbox, classes=cls, scores=score, category_index=category_index, figsize=(12, 16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
