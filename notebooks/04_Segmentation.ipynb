{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "\n",
    "- In out problem, need to detect the nearest pedestrian road to see if the car is violating or not \n",
    "- need to know which part is the pedestrian road and which part is the car road  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import mmcv\n",
    "import torch\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "repo_path = Path(\".\").absolute().parent\n",
    "\n",
    "if os.system == \"nt\":\n",
    "    data_path = Path(\"D:\\Datas\\parking_violation\")\n",
    "else:\n",
    "    data_path = repo_path.parent / \"data\" / \"parking_violation\"\n",
    "sys.path.append(str(repo_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "package repo tree\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   ├── mmseg\n",
    "│   │   └── checkpoints\n",
    "│   └── parking_violation\n",
    "├── mmsegmentation\n",
    "│   └── configs\n",
    "│       └── resnest\n",
    "└── parking_violation\n",
    "    ├── utils.py\n",
    "    └── notebooks\n",
    "        └── 04_Segmentation.ipynb\n",
    "```\n",
    "\n",
    "don't forget to download the weight first\n",
    "\n",
    "**backbone: resnest**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/resnest/deeplabv3plus_s101-d8_512x1024_80k_cityscapes/deeplabv3plus_s101-d8_512x1024_80k_cityscapes_20200807_144429-1239eb43.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```\n",
    "\n",
    "**backbone: R-18-D8**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r18-d8_512x1024_80k_cityscapes/deeplabv3plus_r18-d8_512x1024_80k_cityscapes_20201226_080942-cff257fe.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmseg\n",
    "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
    "from mmseg.core.evaluation import get_palette\n",
    "from mmseg.datasets import CityscapesDataset\n",
    "from utils import imsaver, correct_rgb, resize_image\n",
    "\n",
    "model_config = \"deeplabv3plus\"\n",
    "backbone_config = \"resnest\" # \"r18-d8\" \n",
    "backbone_dict = {\n",
    "    \"r18-d8\": {\n",
    "        \"config_dir\": \"deeplabv3plus\",\n",
    "        \"config\": \"deeplabv3plus_r18-d8_512x1024_80k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_r18-d8_512x1024_80k_cityscapes_20201226_080942-cff257fe.pth\"\n",
    "    },\n",
    "    \"resnest\": {\n",
    "        \"config_dir\": \"resnest\",\n",
    "        \"config\": \"deeplabv3plus_s101-d8_512x1024_80k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_s101-d8_512x1024_80k_cityscapes_20200807_144429-1239eb43.pth\"\n",
    "    } \n",
    "}\n",
    "\n",
    "\n",
    "config_path = repo_path.parent / \"mmsegmentation\" / \"configs\" / backbone_dict[backbone_config][\"config_dir\"]\n",
    "checkpoint_path = repo_path.parent / \"data\" / \"mmseg\" / \"checkpoints\"\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path.mkdir(parents=True)\n",
    "\n",
    "config_file = str(config_path / backbone_dict[backbone_config][\"config\"])\n",
    "checkpoint_file = str(checkpoint_path / backbone_dict[backbone_config][\"checkpoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess video: Origin size is 720x1280\n",
    "height, width = 480, 640\n",
    "# height, width = 720, 1280\n",
    "video_path = str(data_path / \"origin\" / \"sample1.mp4\")\n",
    "resized_video_path = str(data_path / f\"sample1_{height}x{width}.mp4\")\n",
    "resized_frames_path = data_path / f\"sample1_{height}x{width}\"\n",
    "if not resized_frames_path.exists():\n",
    "    resized_frames_path.mkdir()\n",
    "\n",
    "frames_path = resized_frames_path / \"img_dir\"\n",
    "# uncomment underline to resize video\n",
    "# mmcv.resize_video(video_path, resized_video_path, (width, height))\n",
    "# video = mmcv.VideoReader(resized_video_path)\n",
    "# video.cvt2frames(frames_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_segmentor(config_file, checkpoint_file, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets import build_dataloader, CustomDataset\n",
    "from mmseg.apis.inference import LoadImage\n",
    "from mmseg.datasets.pipelines import Compose\n",
    "from mmcv.parallel import collate, scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_inference_segmentor(model, imgs, test_pipeline):\n",
    "    \"\"\"Inference image(s) with the segmentor.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded segmentor.\n",
    "        imgs (list[str/ndarray]): Either image files or loaded images.\n",
    "        \n",
    "    Returns:\n",
    "        (list[Tensor]): The segmentation result.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(imgs, list), \"Must be a list type, contains images\"\n",
    "    device = next(model.parameters()).device  # model device\n",
    "    \n",
    "    # prepare data\n",
    "    res = []\n",
    "    for img in imgs:\n",
    "        data = dict(img=img)\n",
    "        data = test_pipeline(data)\n",
    "        res.append(data)\n",
    "    data = collate(res, samples_per_gpu=len(imgs))\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter to specified GPU\n",
    "        data = scatter(data, [device])[0]\n",
    "    else:\n",
    "        data['img_metas'] = [i.data[0] for i in data['img_metas']]\n",
    "        \n",
    "    # TODO: apply CRF in batch inference\n",
    "    with torch.no_grad():\n",
    "        result = model(return_loss=False, rescale=True, **data)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = model.cfg\n",
    "cfg.data.test.type = \"Cityscapes\"\n",
    "cfg.data.workers_per_gpu = 1\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(2048, 1024),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='Normalize', **img_norm_cfg),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img']),            \n",
    "        ]\n",
    "    )\n",
    "]\n",
    "cfg.data.test.pipeline = test_pipeline\n",
    "# dataset = CustomDataset(\n",
    "#     cfg.data.test.pipeline, test_path, \n",
    "#     classes=CityscapesDataset.CLASSES, \n",
    "#     palette=CityscapesDataset.PALETTE,\n",
    "#     test_mode=True)\n",
    "# data_loader = build_dataloader(dataset, samples_per_gpu=3, workers_per_gpu=1, dist=False, shuffle=False, \n",
    "#                                dataloader_type=\"DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_frame_loader(video, batch_size=4, preprocess=None, *args):\n",
    "    # TODO: change video --> available for str files or numpy arrays\n",
    "    n = len(video) // batch_size\n",
    "    for i in range(n):\n",
    "        if i == n-1:\n",
    "            batches = video[i:n]\n",
    "        else:\n",
    "            batches = video[i*batch_size:(i*batch_size+batch_size)]\n",
    "        if preprocess:\n",
    "            batches = list(map(preprocess, batches))\n",
    "        yield batches\n",
    "        \n",
    "def preprocessor(x, ratio):\n",
    "    # keep the certain height of image\n",
    "    h, w, _ = x.shape\n",
    "    assert ratio > 0.0 and ratio <= 1.0, \"ratio must between (0.0, 1.0]\"\n",
    "    return x[:int(ratio*h)]\n",
    "\n",
    "def post_process(x, pleft=0, ptop=30, pright=200, pbottom=30):\n",
    "    h, w, _ = x.shape\n",
    "    img = mmcv.impad(x, padding=(pleft, ptop, pright, pbottom), pad_val=0)\n",
    "    # pts: lu, ld, rd, ru & each point = (x, y)\n",
    "    pts = [np.array([(w,ptop),(w,ptop+h),(w+pright,ptop+h),(w+pright,ptop)])]\n",
    "    cv2.fillPoly(img, pts=pts, color=(255,255,255))\n",
    "    for i, (cls, clr) in enumerate(class2color.items()):\n",
    "        percent = i/len(class2color)\n",
    "        pos_x = int(w + 0.1*pright)\n",
    "        pos_y = int(percent*h + ptop) + int(h*0.025)\n",
    "        cv2.circle(img, (pos_x, pos_y), int(h*0.01), clr, thickness=-1)\n",
    "        cv2.putText(img, cls, (pos_x+int(h*0.025), pos_y+3), cv2.FONT_HERSHEY_SIMPLEX, h*0.001, color=(0,0,0))\n",
    "\n",
    "    return img\n",
    "\n",
    "output_dir = resized_frames_path / \"output\" / backbone_config\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "video = mmcv.VideoReader(resized_video_path)\n",
    "\n",
    "batch_size = 4\n",
    "ratio = 0.65\n",
    "test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "class2color = dict(zip(*[CityscapesDataset.CLASSES, CityscapesDataset.PALETTE]))\n",
    "pleft, ptop, pright, pbottom = 0, 50, 150, 50\n",
    "bgr_palette = np.array(CityscapesDataset.PALETTE)[..., ::-1]  # need to convert to bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/906 [00:00<?, ?it/s]/home/simonjisu/miniconda3/lib/python3.7/site-packages/mmseg/models/segmentors/base.py:267: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
      "  warnings.warn('show==False and out_file is not specified, only '\n",
      "Processing: 100%|█████████▉| 905/906 [49:05<00:03,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "data_loader = batch_frame_loader(video, batch_size=batch_size, preprocess=lambda x: preprocessor(x, ratio))\n",
    "\n",
    "idx = 1\n",
    "for frames in tqdm(data_loader, total=(len(video)//batch_size + 1), desc=\"Processing\"):\n",
    "    # TODO: apply CRF in batch inference\n",
    "    result = batch_inference_segmentor(model, frames, test_pipeline)\n",
    "    torch.cuda.empty_cache()\n",
    "    for frame, res in zip(frames, result):\n",
    "        overlay_img = model.show_result(frame, [res], palette=bgr_palette, show=False)\n",
    "        img = post_process(overlay_img)\n",
    "        imsaver(img, str(output_dir / f\"sample1-{idx:06d}.jpg\"))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/3617 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/simonjisu/code/data/parking_violation/sample1_480x640_resnest_result.mp4.\n",
      "Moviepy - Writing video /home/simonjisu/code/data/parking_violation/sample1_480x640_resnest_result.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/simonjisu/code/data/parking_violation/sample1_480x640_resnest_result.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import concatenate_videoclips, ImageClip\n",
    "imgs = sorted(list(map(str, output_dir.glob(\"*.jpg\"))))\n",
    "\n",
    "clips = [ImageClip(m).set_duration(1/30) for m in imgs]\n",
    "video_output_path = str(data_path / f\"sample1_{height}x{width}_{backbone_config}_result.mp4\")\n",
    "\n",
    "concat_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "concat_clip.write_videofile(video_output_path, fps=30) #int(video.fps/step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as wd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"300\" controls>\n",
       "  <source src=\"/home/simonjisu/code/data/parking_violation/sample1_720x1280-result.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"300\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**To be updated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Segmentation on Video\n",
    "\n",
    "need to post process using CRF, apply it when on batch outcomes to classify the final output\n",
    "\n",
    "### Conditional Random Field\n",
    "\n",
    "CRF(Conditional Random Field) is a softmax\n",
    "\n",
    "For example, we have images $X_i: (H, W, C)$ and segmented mask $Y_i: (H, W, K)$, $K$ is the number of classes to classify\n",
    "\n",
    "### Maximum Entropy Markov Model (MEMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crfseg import CRF\n",
    "\n",
    "def freeeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = init_segmentor(config_file, checkpoint_file, device=\"cuda:0\")\n",
    "cfg = model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityspaces_path = repo_path.parent / \"data\" / \"cityscapes\"\n",
    "train_dataset = CityscapesDataset(data_root=cityspaces_path, pipeline=cfg.train_pipeline, img_dir=\"leftImg8bit/train\", test_mode=False)\n",
    "val_dataset = CityscapesDataset(data_root=cityspaces_path, pipeline=cfg.test_pipeline, img_dir=\"leftImg8bit/val\", test_mode=True)\n",
    "test_dataset = CityscapesDataset(data_root=cityspaces_path, pipeline=cfg.test_pipeline, img_dir=\"leftImg8bit/test\", test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRFmodel = CRF(n_spatial_dims=len(train_dataset.CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets import build_dataloader, build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_logit = model.inference(data['img'][0], data['img_metas'][0], rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
