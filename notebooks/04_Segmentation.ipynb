{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "\n",
    "- In out problem, need to detect the nearest pedestrian road to see if the car is violating or not \n",
    "- need to know which part is the pedestrian road and which part is the car road  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import mmcv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import ffmpeg\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from crfseg import CRF\n",
    "\n",
    "repo_path = Path(\".\").absolute().parent\n",
    "\n",
    "if os.system == \"nt\":\n",
    "    data_path = Path(\"D:\\Datas\\parking_violation\")\n",
    "else:\n",
    "    data_path = repo_path.parent / \"data\" / \"parking_violation\"\n",
    "sys.path.append(str(repo_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "package repo tree\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   ├── mmseg\n",
    "│   │   └── checkpoints\n",
    "│   └── parking_violation\n",
    "├── mmsegmentation\n",
    "│   └── configs\n",
    "│       └── resnest\n",
    "└── parking_violation\n",
    "    ├── utils.py\n",
    "    └── notebooks\n",
    "        └── 04_Segmentation.ipynb\n",
    "```\n",
    "\n",
    "don't forget to download the weight first\n",
    "\n",
    "**backbone: resnest**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/resnest/deeplabv3plus_s101-d8_512x1024_80k_cityscapes/deeplabv3plus_s101-d8_512x1024_80k_cityscapes_20200807_144429-1239eb43.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```\n",
    "\n",
    "**backbone: R-18-D8**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r18-d8_512x1024_80k_cityscapes/deeplabv3plus_r18-d8_512x1024_80k_cityscapes_20201226_080942-cff257fe.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```\n",
    "\n",
    "**backbone: R-50-D8**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r50-d8_512x1024_40k_cityscapes/deeplabv3plus_r50-d8_512x1024_40k_cityscapes_20200605_094610-d222ffcd.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
    "from mmseg.datasets import CityscapesDataset\n",
    "\n",
    "model_config = \"deeplabv3plus\"\n",
    "backbone_config = \"r50-d8\"\n",
    "backbone_dict = {\n",
    "    \"r50-d8\": {\n",
    "        \"config_dir\": \"deeplabv3plus\",\n",
    "        \"config\": \"deeplabv3plus_r50-d8_512x1024_40k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_r50-d8_512x1024_40k_cityscapes_20200605_094610-d222ffcd.pth\"\n",
    "    },\n",
    "    \"r18-d8\": {\n",
    "        \"config_dir\": \"deeplabv3plus\",\n",
    "        \"config\": \"deeplabv3plus_r18-d8_512x1024_80k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_r18-d8_512x1024_80k_cityscapes_20201226_080942-cff257fe.pth\"\n",
    "    },\n",
    "    \"resnest\": {\n",
    "        \"config_dir\": \"resnest\",\n",
    "        \"config\": \"deeplabv3plus_s101-d8_512x1024_80k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_s101-d8_512x1024_80k_cityscapes_20200807_144429-1239eb43.pth\"\n",
    "    } \n",
    "}\n",
    "\n",
    "\n",
    "config_path = repo_path.parent / \"mmsegmentation\" / \"configs\" / backbone_dict[backbone_config][\"config_dir\"]\n",
    "checkpoint_path = repo_path.parent / \"data\" / \"mmseg\" / \"checkpoints\"\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path.mkdir(parents=True)\n",
    "\n",
    "config_file = str(config_path / backbone_dict[backbone_config][\"config\"])\n",
    "checkpoint_file = str(checkpoint_path / backbone_dict[backbone_config][\"checkpoint\"])\n",
    "crf_checkpoint_file = str(checkpoint_path / f\"crf_{backbone_config}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess video: Origin size is 720x1280\n",
    "height, width = 480, 640\n",
    "video_name = \"sample2\"\n",
    "# height, width = 720, 1280\n",
    "video_path = str(data_path / \"origin\" / f\"{video_name}.mp4\")\n",
    "resized_video_path = str(data_path / f\"{video_name}_{height}x{width}.mp4\")\n",
    "resized_frames_path = data_path / f\"{video_name}_{height}x{width}\"\n",
    "if not resized_frames_path.exists():\n",
    "    resized_frames_path.mkdir()\n",
    "\n",
    "# uncomment underline to resize video\n",
    "# mmcv.resize_video(video_path, resized_video_path, (width, height))\n",
    "\n",
    "# uncomment underline to preprocessing\n",
    "frames_path = resized_frames_path / \"img_dir\"\n",
    "# if list(frames_path.glob(\"*.jpg\")):\n",
    "#     for file in frames_path.glob(\"*.jpg\"):\n",
    "#         file.unlink()\n",
    "        \n",
    "# video = mmcv.VideoReader(resized_video_path)\n",
    "\n",
    "# ratio = 0.65\n",
    "# bboxes = np.array([0, 0, width, int(height*ratio)])\n",
    "# for i, (frame) in tqdm(enumerate(video, 1), total=len(video), desc=\"Extracting\"):\n",
    "#     frame = mmcv.imcrop(frame, bboxes)\n",
    "#     mmcv.imwrite(frame, str(frames_path / f\"{i:06d}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets import build_dataloader, CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegModelCRF(nn.Module):\n",
    "    def __init__(self, config_file, model_checkpoint_file, crf_checkpoint_file, device):\n",
    "        super().__init__()\n",
    "        self.d = device\n",
    "        self.seg_model = init_segmentor(config_file, model_checkpoint_file, device=device)\n",
    "        self.cfg = self.seg_model.cfg\n",
    "        \n",
    "        self.crf = CRF(n_spatial_dims=2, returns=\"log-proba\").to(device)\n",
    "        self.crf.load_state_dict(torch.load(crf_checkpoint_file))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        img = data[\"img\"][0].data.to(self.d)\n",
    "        img_meta = data[\"img_metas\"][0].data[0]\n",
    "        x = self.seg_model.inference(img, img_meta, rescale=True)\n",
    "        x = self.crf(x, display_tqdm=False)\n",
    "        x = x.argmax(1)\n",
    "        return x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegModelCRF(config_file, checkpoint_file, crf_checkpoint_file, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-19 13:55:02,463 - mmseg - INFO - Loaded 3622 images\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "\n",
    "pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(width, height),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='Normalize', **img_norm_cfg),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img']),            \n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset = CustomDataset(\n",
    "    pipeline=pipeline,\n",
    "    img_dir=\"img_dir\",\n",
    "    data_root=str(resized_frames_path), \n",
    "    classes=CityscapesDataset.CLASSES, \n",
    "    palette=CityscapesDataset.PALETTE,\n",
    "    test_mode=True)\n",
    "\n",
    "data_loader = build_dataloader(\n",
    "    dataset, samples_per_gpu=batch_size, \n",
    "    workers_per_gpu=1, dist=False, shuffle=False, dataloader_type=\"DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing::   0%|          | 0/3624 [00:00<?, ?it/s]/home/simonjisu/miniconda3/lib/python3.7/site-packages/crfseg/model.py:91: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  x = negative_unary - x\n",
      "/home/simonjisu/miniconda3/lib/python3.7/site-packages/mmseg/models/segmentors/base.py:267: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
      "  warnings.warn('show==False and out_file is not specified, only '\n",
      "Processing:: 100%|██████████| 3624/3624 [08:08<00:00,  8.67it/s]"
     ]
    }
   ],
   "source": [
    "def post_process(x, pleft=0, ptop=30, pright=200, pbottom=30):\n",
    "    h, w, _ = x.shape\n",
    "    img = mmcv.impad(x, padding=(pleft, ptop, pright, pbottom), pad_val=0)\n",
    "    # pts: lu, ld, rd, ru & each point = (x, y)\n",
    "    pts = [np.array([(w,ptop),(w,ptop+h),(w+pright,ptop+h),(w+pright,ptop)])]\n",
    "    cv2.fillPoly(img, pts=pts, color=(255,255,255))\n",
    "    for i, (cls, clr) in enumerate(class2color.items()):\n",
    "        percent = i/len(class2color)\n",
    "        pos_x = int(w + 0.1*pright)\n",
    "        pos_y = int(percent*h + ptop) + int(h*0.025)\n",
    "        cv2.circle(img, (pos_x, pos_y), int(h*0.01), clr, thickness=-1)\n",
    "        cv2.putText(img, cls, (pos_x+int(h*0.025), pos_y+3), cv2.FONT_HERSHEY_SIMPLEX, h*0.002, color=(0,0,0))\n",
    "\n",
    "    return img\n",
    "\n",
    "get_img_path = lambda x: [m.get(\"filename\") for m in x[\"img_metas\"][0].data[0]]\n",
    "\n",
    "output_dir = resized_frames_path / \"output\" / backbone_config\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "if list(output_dir.glob(\"*.jpg\")):\n",
    "    for file in output_dir.glob(\"*.jpg\"):\n",
    "        file.unlink()    \n",
    "\n",
    "class2color = dict(zip(*[dataset.CLASSES, dataset.PALETTE]))\n",
    "pleft, ptop, pright, pbottom = 0, 50, 150, 50\n",
    "bgr_palette = np.array(dataset.PALETTE)[..., ::-1]  # need to convert to bgr\n",
    "\n",
    "pbar = tqdm(desc=\"Processing:\", total=len(data_loader)*batch_size)\n",
    "\n",
    "for data in data_loader:\n",
    "    result = model(data)\n",
    "    torch.cuda.empty_cache()\n",
    "    for img_p, res in zip(get_img_path(data), result):\n",
    "        frame = mmcv.bgr2rgb(mmcv.imread(img_p))\n",
    "        overlay_img = model.seg_model.show_result(frame, [res], palette=bgr_palette, show=False)\n",
    "        img = post_process(overlay_img)\n",
    "        \n",
    "        # 이미지 이름에 맞춰서 저장 img_p\n",
    "        mmcv.imwrite(img, str(output_dir / f\"{video_name}-{Path(img_p).name}\"))\n",
    "    pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import concatenate_videoclips, ImageClip\n",
    "imgs = sorted(list(map(str, output_dir.glob(\"*.jpg\"))))\n",
    "\n",
    "clips = [ImageClip(m).set_duration(1/30) for m in imgs]\n",
    "video_output_path = str(data_path / f\"sample1_{height}x{width}_{backbone_config}_result.mp4\")\n",
    "\n",
    "concat_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "concat_clip.write_videofile(video_output_path, fps=30) #int(video.fps/step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as wd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"300\" controls>\n",
       "  <source src=\"/home/simonjisu/code/data/parking_violation/sample1_720x1280-result.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"300\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**To be updated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Segmentation on Video\n",
    "\n",
    "Problem: Image at each frame contains a lot of noise. Need some post-processing before get the final output.\n",
    "\n",
    "### Conditional Random Field\n",
    "\n",
    "#### CRF as CNN\n",
    "\n",
    "- Paper: https://arxiv.org/abs/1210.5644\n",
    "- Article Refernece: https://arxiv.org/abs/1502.03240\n",
    "\n",
    "<img src=\"./imgs/04_CRF02.png\" width=480>\n",
    "\n",
    "A CRF, used in the context of pixel-wise label prediction, models pixel labels as random variables that form a Markov Random Field(MRF) when conditioned upon a global observation. The global observation is usually taken to be the image.\n",
    "\n",
    "* Let $X_i$ be the random variable associated to pixel $i$, which represents the label assigned to the pixel $i$ and\n",
    "can take any value from a pre-defined set of labels $L = \\{l_1, l_2, \\cdots , l_L\\}$. \n",
    "* Let $X$ be the vector formed by the random variables $X_1, X_2, \\cdots , X_N$ , where $N$ is the number of pixels in the image. \n",
    "* Given a graph $G = (V, E)$, where $V = \\{X_1, X_2, \\cdots , X_N \\}$, and a global observation (image) $I$, the pair $(I, X)$ can be modelled as a CRF characterized by a Gibbs distribution of the form $P(X = x|I) = \\dfrac{1}{Z(I)}\\exp\\big(−E(x|I)\\big)$. \n",
    "* Here $E(x)$ is called the energy of the configuration $x \\in \\mathcal{L}^N$ and $Z(I)$ is the partition function.\n",
    "* From now on, we drop the conditioning on $I$ in the notation for convenience.\n",
    "* In the fully connected pairwise CRF model of [29](https://arxiv.org/abs/1210.5644), [pdf](http://swoh.web.engr.illinois.edu/courses/IE598/handout/fall2016_slide15.pdf), the energy of a label assignment x is given by: \n",
    "    $$ E(x) = \\sum_{i}{\\psi}_u (x_i) + \\sum_{i < j} \\psi_p (x_i, x_j)$$\n",
    "    * where the unary energy components $\\psi_u(x_i)$ measure the inverse likelihood (and therefore, the cost) of the pixel $i$ taking the label $x_i$ $\\rightarrow$ label score\n",
    "    * pairwise energy components $\\psi_p(x_i, x_j)$ measure the cost of assigning labels $x_i, x_j$ to pixels $i, j$ simultaneously $\\rightarrow$ data-dependent smoothing term that encourages assiging similar lavels to pixels with similar properties.\n",
    "*  As was done in [29](https://arxiv.org/abs/1210.5644), we model pairwise potentials as weighted Gaussians:\n",
    "    $$ \\psi_p(x_i, x_j) = \\mu(x_i, x_j) \\sum_{m=1}^M w^{(m)}k_G^{(m)}(f_i, f_j)$$\n",
    "    * where each $k^{(m)}_G$ for $m = 1, \\cdots, M$ is a Gaussian kernel applied on feature vectors\n",
    "    * The feature vector of pixel $i$, denoted by $f_i$, is derived from image features such as spatial location and RGB values [29].\n",
    "    * The function $µ(., .)$, called the label compatibility function, captures the compatibility between different pairs of labels as the name implies.\n",
    "* Minimizing the above CRF energy $E(x)$ yields the most probable label assignment $x$ for the given image. Since this exact minimization is intractable, a mean-field approximation to the CRF distribution is used for approximate maximum posterior marginal inference.\n",
    "*  It consists in approximating the CRF distribution $P(X)$ by a simpler distribution $Q(X)$, which can be written as the product of independent marginal distributions, i.e., $Q(X) = \\prod_{i} Q_i(X_i)$.\n",
    "\n",
    "<img src=\"./imgs/04_CRF01.png\" width=480>\n",
    "\n",
    "* input=unary potentials: (B, K, *spatial)\n",
    "* Normalizing: softmax(dim=1)\n",
    "* Message Passing: apply Gaussian Kernel for each image (B, K, *spatial) `Conv1d` \n",
    "* Compatibility Transform: torch.einsum(input, compatibility) (B, K, *spatial)\n",
    "    * compatibility: negative identity matrix (K, K)\n",
    "* adding unary potentials\n",
    "\n",
    "#### CRF as RNN\n",
    "\n",
    "- Paper: https://arxiv.org/abs/1502.03240\n",
    "- Code: https://github.com/sadeepj/crfasrnn_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mmseg.datasets import build_dataloader\n",
    "from mmseg.core.evaluation import mean_iou\n",
    "from crfseg import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-18 13:29:48,577 - mmseg - INFO - Loaded 2975 images\n",
      "2021-01-18 13:29:48,585 - mmseg - INFO - Loaded 500 images\n"
     ]
    }
   ],
   "source": [
    "cityspaces_path = repo_path.parent / \"data\" / \"cityscapes\"\n",
    "device = \"cuda\"\n",
    "batch_size = 2\n",
    "\n",
    "seg_model = init_segmentor(config_file, checkpoint_file, device=device)\n",
    "crf = CRF(n_spatial_dims=2, returns=\"log-proba\").to(device)\n",
    "\n",
    "cfg = seg_model.cfg\n",
    "train_dataset = CityscapesDataset(\n",
    "    data_root=cityspaces_path, \n",
    "    pipeline=cfg.data.train.pipeline, \n",
    "    img_dir=cfg.data.train.img_dir, \n",
    "    ann_dir=cfg.data.train.ann_dir, \n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "val_dataset = CityscapesDataset(\n",
    "    data_root=cityspaces_path, \n",
    "    pipeline=cfg.data.val.pipeline, \n",
    "    img_dir=cfg.data.val.img_dir, \n",
    "    ann_dir=cfg.data.val.ann_dir, \n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "train_loader = build_dataloader(\n",
    "    train_dataset, samples_per_gpu=batch_size, workers_per_gpu=0, dataloader_type=\"DataLoader\")\n",
    "\n",
    "val_loader = build_dataloader(\n",
    "    val_dataset, samples_per_gpu=batch_size, workers_per_gpu=0, dataloader_type=\"DataLoader\")\n",
    "\n",
    "# Freeze Seg Network\n",
    "for param in seg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(crf.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluations(log_proba, seg, n_classes, ignore_idx):\n",
    "    res = log_proba.argmax(1)\n",
    "    miou, cate_acc, cate_iou = mean_iou(\n",
    "        results=res,\n",
    "        gt_seg_maps=seg, \n",
    "        num_classes=n_classes, \n",
    "        ignore_index=ignore_idx, \n",
    "        nan_to_num=0.0\n",
    "    )\n",
    "    # record\n",
    "    return miou, cate_acc, cate_iou\n",
    "\n",
    "def train_crf(train_loader, seg_model, crf, optimizer):\n",
    "    batch_size = train_loader.batch_size\n",
    "    train_dst = train_loader.dataset\n",
    "    total_loss = 0\n",
    "    total_miou = 0\n",
    "    total_cate_acc = np.zeros(len(train_dst.CLASSES))\n",
    "    total_cate_iou = np.zeros(len(train_dst.CLASSES))\n",
    "    n_classes = len(train_dst.CLASSES)\n",
    "    ignore_idx = train_dst.ignore_index\n",
    "    n = 0\n",
    "    \n",
    "    pbar = tqdm(desc=\"[Train]\", total=len(train_loader)*batch_size)\n",
    "    seg_model.eval()\n",
    "    crf.train()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        img = data[\"img\"].data[0].to(device)\n",
    "        img_meta = data[\"img_metas\"].data[0]\n",
    "        seg = data[\"gt_semantic_seg\"].data[0].squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x = seg_model.inference(img, img_meta, rescale=False)\n",
    "        log_proba = crf(x, display_tqdm=False)  # (B, K, H, W)\n",
    "        loss = F.nll_loss(log_proba, seg, ignore_index=ignore_idx, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluations\n",
    "        miou, cate_acc, cate_iou = get_evaluations(\n",
    "            log_proba.cpu(), seg.cpu(), n_classes, ignore_idx\n",
    "        )\n",
    "\n",
    "        # record\n",
    "        total_loss += loss.item()\n",
    "        total_miou += miou\n",
    "        total_cate_acc += cate_acc.round(4)\n",
    "        total_cate_iou += cate_iou.round(4)\n",
    "        \n",
    "        n += 1\n",
    "\n",
    "        pbar.update(batch_size)\n",
    "        pbar.set_description(f\"[Train] Loss {loss.item():.4f} | Mean IoU {miou:.4f}\")\n",
    "    pbar.close()\n",
    "    \n",
    "    return crf, total_loss/n, total_miou/n, total_cate_acc/n, total_cate_iou/n\n",
    "\n",
    "def val_crf(val_loader, seg_model, crf):\n",
    "    batch_size = val_loader.batch_size\n",
    "    val_dst = val_loader.dataset\n",
    "    total_loss = 0\n",
    "    total_miou = 0\n",
    "    total_cate_acc = np.zeros(len(val_dst.CLASSES))\n",
    "    total_cate_iou = np.zeros(len(val_dst.CLASSES))\n",
    "    n_classes = len(val_dst.CLASSES)\n",
    "    ignore_idx = val_dst.ignore_index\n",
    "    n = 0\n",
    "    \n",
    "    pbar = tqdm(desc=\"[Valid]\", total=len(val_loader)*batch_size)\n",
    "    seg_model.eval()\n",
    "    crf.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            img = data[\"img\"][0].data.to(device)\n",
    "            img_meta = data[\"img_metas\"][0].data[0]\n",
    "            seg = data[\"gt_semantic_seg\"][0].data.squeeze(1).long().to(device)\n",
    "\n",
    "            x = seg_model.inference(img, img_meta, rescale=False)\n",
    "            log_proba = crf(x, display_tqdm=False)\n",
    "            loss = F.nll_loss(log_proba, seg, ignore_index=ignore_idx, reduction=\"mean\")\n",
    "            # evaluations\n",
    "            miou, cate_acc, cate_iou = get_evaluations(\n",
    "                log_proba.cpu(), seg.cpu(), n_classes, ignore_idx\n",
    "            )\n",
    "            # record\n",
    "            total_loss += loss.item()\n",
    "            total_miou += miou\n",
    "            total_cate_acc += cate_acc.round(4)\n",
    "            total_cate_iou += cate_iou.round(4)\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            pbar.update(batch_size)\n",
    "            pbar.set_description(f\"[Valid] Loss {loss.item():.4f} | Mean IoU {miou:.4f}\")\n",
    "    pbar.close()\n",
    "    \n",
    "    return crf, total_loss/n, total_miou/n, total_cate_acc/n, total_cate_iou/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]/home/simonjisu/miniconda3/lib/python3.7/site-packages/crfseg/model.py:91: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  x = negative_unary - x\n",
      "/home/simonjisu/miniconda3/lib/python3.7/site-packages/mmseg/core/evaluation/metrics.py:162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc = total_area_intersect / total_area_label\n",
      "/home/simonjisu/miniconda3/lib/python3.7/site-packages/mmseg/core/evaluation/metrics.py:166: RuntimeWarning: invalid value encountered in true_divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "[Train] Loss 2.2906 | Mean IoU 0.4828: 100%|██████████| 2976/2976 [23:16<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.4199 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:45<00:00,  1.17s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss: 1.3928 | Mean IoU: 0.7379\n",
      "[1/10] Loss: 0.4485 | Mean IoU: 0.9623\n",
      "[INFO] Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 0.6419 | Mean IoU 0.9261: 100%|██████████| 2976/2976 [23:19<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3212 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:08<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10] Loss: 1.3277 | Mean IoU: 0.7355\n",
      "[2/10] Loss: 0.3508 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 1.7770 | Mean IoU 0.6095: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3996 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10] Loss: 1.3079 | Mean IoU: 0.7407\n",
      "[3/10] Loss: 0.4284 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 0.5484 | Mean IoU 0.9490: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.4171 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10] Loss: 1.3131 | Mean IoU: 0.7389\n",
      "[4/10] Loss: 0.4458 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 0.4869 | Mean IoU 0.9488: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3791 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10] Loss: 1.3297 | Mean IoU: 0.7350\n",
      "[5/10] Loss: 0.4081 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 2.4368 | Mean IoU 0.4419: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3880 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10] Loss: 1.2874 | Mean IoU: 0.7453\n",
      "[6/10] Loss: 0.4170 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 1.0909 | Mean IoU 0.7770: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3463 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10] Loss: 1.3039 | Mean IoU: 0.7417\n",
      "[7/10] Loss: 0.3757 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 0.4965 | Mean IoU 0.9584: 100%|██████████| 2976/2976 [23:16<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.4106 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10] Loss: 1.3246 | Mean IoU: 0.7360\n",
      "[8/10] Loss: 0.4394 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 0.3488 | Mean IoU 0.9626: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3359 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:09<00:00,  1.10s/it]\n",
      "[Train]:   0%|          | 0/2976 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10] Loss: 1.3242 | Mean IoU: 0.7359\n",
      "[9/10] Loss: 0.3654 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss 1.3293 | Mean IoU 0.7208: 100%|██████████| 2976/2976 [23:17<00:00,  2.13it/s]\n",
      "[Valid] Loss 0.3012 | Mean IoU 0.9671: 100%|██████████| 500/500 [09:07<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] Loss: 1.2954 | Mean IoU: 0.7429\n",
      "[10/10] Loss: 0.3309 | Mean IoU: 0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'closse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e0a97d3eca31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0meval_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{acc:.4f}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0macc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_cate_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0meval_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{iou:.4f}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miou\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_cate_iou\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0meval_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'closse'"
     ]
    }
   ],
   "source": [
    "n_step = 5\n",
    "best_miou = 0.0\n",
    "best_loss = 999999\n",
    "crf_checkpoint_file = f\"crf_{backbone_config}.pth\"\n",
    "sv_eval_path = Path(\"./eval_result.txt\")\n",
    "eval_file = sv_eval_path.open(\"w\", encoding=\"utf-8\")\n",
    "eval_file.write(\"\\t\".join(train_dataset.CLASSES))\n",
    "\n",
    "for step in range(n_step):\n",
    "    crf, train_loss, train_miou, train_cate_acc, train_cate_iou = train_crf(\n",
    "        train_loader, seg_model, crf, optimizer)\n",
    "    torch.cuda.empty_cache()\n",
    "    crf, val_loss, val_miou, val_cate_acc, val_cate_iou = val_crf(\n",
    "        val_loader, seg_model, crf)\n",
    "    print(f\"[{step+1}/{n_step}] Train Loss: {train_loss:.4f} | Train Mean IoU: {train_miou:.4f}\")\n",
    "    print(f\"[{step+1}/{n_step}] Val Loss: {val_loss:.4f} | Val Mean IoU: {val_miou:.4f}\")\n",
    "    # Save \n",
    "    if best_loss > val_loss:\n",
    "        best_loss = val_loss\n",
    "        best_miou = val_miou\n",
    "        best_cate_acc = val_cate_acc\n",
    "        best_cate_iou = val_cate_iou\n",
    "        torch.save(crf.state_dict(), crf_checkpoint_file)\n",
    "        print(\"[INFO] Saved\")\n",
    "        \n",
    "eval_file.write(\"\\t\".join([f\"{acc:.4f}\" for acc in best_cate_acc]))\n",
    "eval_file.write(\"\\t\".join([f\"{iou:.4f}\" for iou in best_cate_iou]))\n",
    "eval_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
