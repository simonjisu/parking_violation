{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "\n",
    "- In out problem, need to detect the nearest pedestrian road to see if the car is violating or not \n",
    "- need to know which part is the pedestrian road and which part is the car road  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import mmcv\n",
    "import torch\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "repo_path = Path(\".\").absolute().parent\n",
    "\n",
    "if os.system == \"nt\":\n",
    "    data_path = Path(\"D:\\Datas\\parking_violation\")\n",
    "else:\n",
    "    data_path = repo_path.parent / \"data\" / \"parking_violation\"\n",
    "sys.path.append(str(repo_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "package repo tree\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   ├── mmseg\n",
    "│   │   └── checkpoints\n",
    "│   └── parking_violation\n",
    "├── mmsegmentation\n",
    "│   └── configs\n",
    "│       └── resnest\n",
    "└── parking_violation\n",
    "    ├── utils.py\n",
    "    └── notebooks\n",
    "        └── 04_Segmentation.ipynb\n",
    "```\n",
    "\n",
    "don't forget to download the weight first\n",
    "\n",
    "**backbone: resnest**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/resnest/deeplabv3plus_s101-d8_512x1024_80k_cityscapes/deeplabv3plus_s101-d8_512x1024_80k_cityscapes_20200807_144429-1239eb43.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```\n",
    "\n",
    "**backbone: R-18-D8**\n",
    "\n",
    "```\n",
    "!wget https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r18-d8_512x1024_80k_cityscapes/deeplabv3plus_r18-d8_512x1024_80k_cityscapes_20201226_080942-cff257fe.pth -P ~/code/data/mmseg/checkpoints/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmseg\n",
    "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
    "from mmseg.core.evaluation import get_palette\n",
    "from mmseg.datasets import CityscapesDataset\n",
    "from utils import imsaver, correct_rgb, resize_image\n",
    "\n",
    "model_config = \"deeplabv3plus\"\n",
    "backbone_config = \"resnest\" # \"r18-d8\" \n",
    "backbone_dict = {\n",
    "    \"r18-d8\": {\n",
    "        \"config_dir\": \"deeplabv3plus\",\n",
    "        \"config\": \"deeplabv3plus_r18-d8_512x1024_80k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_r18-d8_512x1024_80k_cityscapes_20201226_080942-cff257fe.pth\"\n",
    "    },\n",
    "    \"resnest\": {\n",
    "        \"config_dir\": \"resnest\",\n",
    "        \"config\": \"deeplabv3plus_s101-d8_512x1024_80k_cityscapes.py\",\n",
    "        \"checkpoint\": \"deeplabv3plus_s101-d8_512x1024_80k_cityscapes_20200807_144429-1239eb43.pth\"\n",
    "    } \n",
    "}\n",
    "\n",
    "\n",
    "config_path = repo_path.parent / \"mmsegmentation\" / \"configs\" / backbone_dict[backbone_config][\"config_dir\"]\n",
    "checkpoint_path = repo_path.parent / \"data\" / \"mmseg\" / \"checkpoints\"\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path.mkdir(parents=True)\n",
    "\n",
    "config_file = str(config_path / backbone_dict[backbone_config][\"config\"])\n",
    "checkpoint_file = str(checkpoint_path / backbone_dict[backbone_config][\"checkpoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess video: Origin size is 720x1280\n",
    "height, width = 480, 640\n",
    "# height, width = 720, 1280\n",
    "video_path = str(data_path / \"origin\" / \"sample1.mp4\")\n",
    "resized_video_path = str(data_path / f\"sample1_{height}x{width}.mp4\")\n",
    "resized_frames_path = data_path / f\"sample1_{height}x{width}\"\n",
    "if not resized_frames_path.exists():\n",
    "    resized_frames_path.mkdir()\n",
    "\n",
    "test_path = resized_frames_path / \"img_dir\" / \"val\"\n",
    "# uncomment underline to resize video\n",
    "# mmcv.resize_video(video_path, resized_video_path, (width, height))\n",
    "# video = mmcv.VideoReader(resized_video_path)\n",
    "# video.cvt2frames(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_segmentor(config_file, checkpoint_file, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets import build_dataloader, CustomDataset\n",
    "from mmseg.apis.inference import LoadImage\n",
    "from mmseg.datasets.pipelines import Compose\n",
    "from mmcv.parallel import collate, scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_inference_segmentor(model, imgs, test_pipeline):\n",
    "    \"\"\"Inference image(s) with the segmentor.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded segmentor.\n",
    "        imgs (list[str/ndarray]): Either image files or loaded images.\n",
    "        \n",
    "    Returns:\n",
    "        (list[Tensor]): The segmentation result.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(imgs, list), \"Must be a list type, contains images\"\n",
    "    device = next(model.parameters()).device  # model device\n",
    "    \n",
    "    # prepare data\n",
    "    res = []\n",
    "    for img in imgs:\n",
    "        data = dict(img=img)\n",
    "        data = test_pipeline(data)\n",
    "        res.append(data)\n",
    "    data = collate(res, samples_per_gpu=len(imgs))\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter to specified GPU\n",
    "        data = scatter(data, [device])[0]\n",
    "    else:\n",
    "        data['img_metas'] = [i.data[0] for i in data['img_metas']]\n",
    "        \n",
    "    # TODO: apply CRF in batch inference\n",
    "    with torch.no_grad():\n",
    "        result = model(return_loss=False, rescale=True, **data)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = model.cfg\n",
    "cfg.data.workers_per_gpu = 1\n",
    "cfg.data.test.img_dir = str(test_path)\n",
    "cfg.data.test.ann_dir = None\n",
    "cfg.data.test.type = \"CustomDataset\"\n",
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(2048, 1024),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='Normalize', **img_norm_cfg),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img']),            \n",
    "        ]\n",
    "    )\n",
    "]\n",
    "cfg.data.test.pipeline = test_pipeline\n",
    "# dataset = CustomDataset(\n",
    "#     cfg.data.test.pipeline, test_path, \n",
    "#     classes=CityscapesDataset.CLASSES, \n",
    "#     palette=CityscapesDataset.PALETTE,\n",
    "#     test_mode=True)\n",
    "# data_loader = build_dataloader(dataset, samples_per_gpu=3, workers_per_gpu=1, dist=False, shuffle=False, \n",
    "#                                dataloader_type=\"DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_frame_loader(video, batch_size=4, preprocess=None, *args):\n",
    "    # TODO: change video --> available for str files or numpy arrays\n",
    "    n = len(video) // batch_size\n",
    "    for i in range(n):\n",
    "        if i == n-1:\n",
    "            batches = video[i:n]\n",
    "        else:\n",
    "            batches = video[i*batch_size:(i*batch_size+batch_size)]\n",
    "        if preprocess:\n",
    "            batches = list(map(preprocess, batches))\n",
    "        yield batches\n",
    "        \n",
    "def preprocessor(x, ratio):\n",
    "    # keep the certain height of image\n",
    "    h, w, _ = x.shape\n",
    "    assert ratio > 0.0 and ratio <= 1.0, \"ratio must between (0.0, 1.0]\"\n",
    "    return x[:int(ratio*h)]\n",
    "\n",
    "def post_process(x, pleft=0, ptop=30, pright=200, pbottom=30):\n",
    "    h, w, _ = x.shape\n",
    "    img = mmcv.impad(x, padding=(pleft, ptop, pright, pbottom), pad_val=0)\n",
    "    # pts: lu, ld, rd, ru & each point = (x, y)\n",
    "    pts = [np.array([(w,ptop),(w,ptop+h),(w+pright,ptop+h),(w+pright,ptop)])]\n",
    "    cv2.fillPoly(img, pts=pts, color=(255,255,255))\n",
    "    for i, (cls, clr) in enumerate(class2color.items()):\n",
    "        percent = i/len(class2color)\n",
    "        pos_x = int(w + 0.1*pright)\n",
    "        pos_y = int(percent*h + ptop) + int(h*0.025)\n",
    "        cv2.circle(img, (pos_x, pos_y), int(h*0.01), clr, thickness=-1)\n",
    "        cv2.putText(img, cls, (pos_x+int(h*0.025), pos_y+3), cv2.FONT_HERSHEY_SIMPLEX, h*0.001, color=(0,0,0))\n",
    "\n",
    "    return img\n",
    "\n",
    "output_dir = resized_frames_path / \"output\" / backbone_config\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "video = mmcv.VideoReader(resized_video_path)\n",
    "\n",
    "batch_size = 4\n",
    "ratio = 0.65\n",
    "test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "class2color = dict(zip(*[CityscapesDataset.CLASSES, CityscapesDataset.PALETTE]))\n",
    "pleft, ptop, pright, pbottom = 0, 50, 200, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/906 [00:00<?, ?it/s]/home/simonjisu/miniconda3/lib/python3.7/site-packages/mmseg/models/segmentors/base.py:267: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
      "  warnings.warn('show==False and out_file is not specified, only '\n",
      "Processing: 100%|█████████▉| 905/906 [50:13<00:03,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "data_loader = batch_frame_loader(video, batch_size=batch_size, preprocess=lambda x: preprocessor(x, ratio))\n",
    "\n",
    "idx = 1\n",
    "for frames in tqdm(data_loader, total=(len(video)//batch_size + 1), desc=\"Processing\"):\n",
    "    # TODO: apply CRF in batch inference\n",
    "    result = batch_inference_segmentor(model, frames, test_pipeline)\n",
    "    torch.cuda.empty_cache()\n",
    "    for frame, res in zip(frames, result):\n",
    "        overlay_img = model.show_result(frame, [res], palette=CityscapesDataset.PALETTE, show=False)\n",
    "        img = post_process(overlay_img)\n",
    "        imsaver(img, str(output_dir / f\"sample1-{idx:06d}.jpg\"))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/3617 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/simonjisu/code/data/parking_violation/sample1_480x640_resnest_result.mp4.\n",
      "Moviepy - Writing video /home/simonjisu/code/data/parking_violation/sample1_480x640_resnest_result.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/simonjisu/code/data/parking_violation/sample1_480x640_resnest_result.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import concatenate_videoclips, ImageClip\n",
    "imgs = sorted(list(map(str, output_dir.glob(\"*.jpg\"))))\n",
    "\n",
    "clips = [ImageClip(m).set_duration(1/30) for m in imgs]\n",
    "video_output_path = str(data_path / f\"sample1_{height}x{width}_{backbone_config}_result.mp4\")\n",
    "\n",
    "concat_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "concat_clip.write_videofile(video_output_path, fps=30) #int(video.fps/step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as wd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_output_path = str(data_path / f\"sample1_{height}x{width}-result.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(video_output_path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"300\" controls>\n",
       "  <source src=\"/home/simonjisu/code/data/parking_violation/sample1_720x1280-result.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"300\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(path):\n",
    "    display(HTML(\"\"\"\n",
    "        <video width=640 height=270 controls>\n",
    "          <source src=\"{}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\".format(path))\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7729a54b2f764472ab9237a3471d25e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='/home/simonjisu/code/data/parking_violation/sample1_360x640/sample1_360x640.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wd.interact(show_video, path = video_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/906 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader = batch_frame_loader(video, batch_size=batch_size, preprocess=lambda x: preprocessor(x, ratio))\n",
    "\n",
    "idx = 1\n",
    "for frames in tqdm(data_loader, total=(len(video)//batch_size + 1), desc=\"Processing\"):\n",
    "    # TODO: apply CRF in batch inference\n",
    "    result = batch_inference_segmentor(model, frames, test_pipeline)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = frames\n",
    "device = next(model.parameters()).device  # model device\n",
    "\n",
    "# prepare data\n",
    "res = []\n",
    "for img in imgs:\n",
    "    data = dict(img=img)\n",
    "    data = test_pipeline(data)\n",
    "    res.append(data)\n",
    "data = collate(res, samples_per_gpu=len(imgs))\n",
    "if next(model.parameters()).is_cuda:\n",
    "    # scatter to specified GPU\n",
    "    data = scatter(data, [device])[0]\n",
    "else:\n",
    "    data['img_metas'] = [i.data[0] for i in data['img_metas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': None,\n",
       " 'ori_filename': None,\n",
       " 'ori_shape': (288, 848, 3),\n",
       " 'img_shape': (696, 2048, 3),\n",
       " 'pad_shape': (696, 2048, 3),\n",
       " 'scale_factor': array([2.4150944, 2.4166667, 2.4150944, 2.4166667], dtype=float32),\n",
       " 'flip': False,\n",
       " 'flip_direction': 'horizontal',\n",
       " 'img_norm_cfg': {'mean': array([123.675, 116.28 , 103.53 ], dtype=float32),\n",
       "  'std': array([58.395, 57.12 , 57.375], dtype=float32),\n",
       "  'to_rgb': True}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['img_metas'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_logit = model.inference(data['img'][0], data['img_metas'][0], rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 288, 848])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_logit[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Segmentation on Video\n",
    "\n",
    "need to post process using CRF, apply it when on batch outcomes to classify the final output\n",
    "\n",
    "### Conditional Random Field\n",
    "\n",
    "CRF(Conditional Random Field) is a softmax\n",
    "\n",
    "For example, we have images $X_i: (H, W, C)$ and segmented mask $Y_i: (H, W, K)$, $K$ is the number of classes to classify\n",
    "\n",
    "### Maximum Entropy Markov Model (MEMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_segmentor(config_file, checkpoint_file, device=\"cuda:0\")\n",
    "cfg = model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeeze(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-02 12:11:18,529 - mmseg - INFO - Loaded 2975 images\n",
      "2021-01-02 12:11:18,537 - mmseg - INFO - Loaded 500 images\n",
      "2021-01-02 12:11:18,558 - mmseg - INFO - Loaded 1525 images\n"
     ]
    }
   ],
   "source": [
    "model = init_segmentor(config_file, checkpoint_file, device=\"cuda:0\")\n",
    "cfg = model.cfg\n",
    "\n",
    "cityspaces_path = repo_path.parent / \"data\" / \"cityscapes\"\n",
    "train_dataset = CityscapesDataset(data_root=cityspaces_path, pipeline=cfg.train_pipeline, img_dir=\"leftImg8bit/train\", test_mode=False)\n",
    "val_dataset = CityscapesDataset(data_root=cityspaces_path, pipeline=cfg.test_pipeline, img_dir=\"leftImg8bit/val\", test_mode=True)\n",
    "test_dataset = CityscapesDataset(data_root=cityspaces_path, pipeline=cfg.test_pipeline, img_dir=\"leftImg8bit/test\", test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crfseg import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRFmodel = CRF(n_spatial_dims=len(train_dataset.CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets import build_dataloader, build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f895eaf177bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/mmseg/datasets/builder.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(cfg, default_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RepeatDataset'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         dataset = RepeatDataset(\n\u001b[1;32m     68\u001b[0m             build_dataset(cfg['dataset'], default_args), cfg['times'])\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/mmcv/utils/config.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/mmcv/utils/config.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "build_dataloader(\n",
    "    dataset,\n",
    "     samples_per_gpu,\n",
    "     workers_per_gpu,\n",
    "     num_gpus=1,\n",
    "     dist=True,\n",
    "     shuffle=True,\n",
    "     seed=None,\n",
    "     drop_last=False,\n",
    "     pin_memory=True,\n",
    "     dataloader_type='PoolDataLoader',\n",
    "     **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
